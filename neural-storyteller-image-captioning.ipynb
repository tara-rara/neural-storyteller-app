{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1111749,"sourceType":"datasetVersion","datasetId":623329,"isSourceIdPinned":false}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"adityajn105/flickr30k\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-12T07:20:29.184202Z","iopub.execute_input":"2026-02-12T07:20:29.184583Z","iopub.status.idle":"2026-02-12T07:20:30.043401Z","shell.execute_reply.started":"2026-02-12T07:20:29.184505Z","shell.execute_reply":"2026-02-12T07:20:30.042550Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part 1 : Feature Extraction Pipeline","metadata":{}},{"cell_type":"code","source":"import os, pickle, torch, torch.nn as nn\nfrom torchvision import models, transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image\nfrom tqdm import tqdm\n\ndef find_image_dir():\n    # Common Kaggle root \n    base_input = '/kaggle/input'\n    # where the images actually are\n    for root, dirs, files in os.walk(base_input):\n        # Looking for the folder containing a high volume of jpg files\n        if len([f for f in files if f.endswith('.jpg')]) > 1000:\n            return root\n    return None\n\nIMAGE_DIR = find_image_dir()\nOUTPUT_FILE = 'flickr30k_features.pkl'\n\nif IMAGE_DIR:\n    print(f\" Found images at: {IMAGE_DIR}\") \nelse:\n    raise FileNotFoundError(\"Could not find the Flickr30k image directory. Please ensure the dataset is added to the notebook.\")\n\n# DATASET CLASS\nclass FlickrDataset(Dataset):\n    def __init__(self, img_dir, transform):\n        self.img_names = [f for f in os.listdir(img_dir) if f.endswith(('.jpg', '.jpeg'))] \n        self.transform = transform \n        self.img_dir = img_dir \n\n    def __len__(self):\n        return len(self.img_names) \n\n    def __getitem__(self, idx):\n        name = self.img_names[idx] \n        img_path = os.path.join(self.img_dir, name)\n        img = Image.open(img_path).convert('RGB') \n        return self.transform(img), name\n\n# FEATURE EXTRACTION PIPELINE\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n\n# Load pre-trained ResNet50 and strip the final classification layer\nmodel = models.resnet50(weights=models.ResNet50_Weights.DEFAULT) \nmodel = nn.Sequential(*list(model.children())[:-1]) # Extract 2048-dim feature vector only \nmodel = nn.DataParallel(model).to(device)\nmodel.eval()\n\n# Image preprocessing transforms as required by ResNet50\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)), \n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) \n])\n\ndataset = FlickrDataset(IMAGE_DIR, transform) \nloader = DataLoader(dataset, batch_size=128, num_workers=4) \nfeatures_dict = {}\n\n# Run the extraction \nwith torch.no_grad():\n    for imgs, names in tqdm(loader, desc=\"Extracting Features\"): \n        # Flatten the output to (batch_size, 2048)\n        feats = model(imgs.to(device)).view(imgs.size(0), -1)\n        for i, name in enumerate(names):\n            features_dict[name] = feats[i].cpu().numpy() \n\n# Save the dictionary to a pickle file \nwith open(OUTPUT_FILE, 'wb') as f:\n    pickle.dump(features_dict, f) \n\nprint(f\"Success! {len(features_dict)} images processed and saved to {OUTPUT_FILE}\") ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T07:20:30.044857Z","iopub.execute_input":"2026-02-12T07:20:30.045246Z","iopub.status.idle":"2026-02-12T07:22:43.097545Z","shell.execute_reply.started":"2026-02-12T07:20:30.045218Z","shell.execute_reply":"2026-02-12T07:22:43.096692Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part 2: Vocabulary & Text Pre-Processing","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\nfrom collections import Counter\n\nclass Vocabulary:\n    def __init__(self, freq_threshold):\n        # 0: padding, 1: start of sentence, 2: end of sentence, 3: unknown words\n        self.itos = {0: \"<pad>\", 1: \"<start>\", 2: \"<end>\", 3: \"<unk>\"}\n        self.stoi = {\"<pad>\": 0, \"<start>\": 1, \"<end>\": 2, \"<unk>\": 3}\n        self.freq_threshold = freq_threshold\n\n    def __len__(self):\n        return len(self.itos)\n\n    @staticmethod\n    def tokenizer(text):\n        # Cleaning the text: lowercase, removing non-alphabetic chars\n        text = str(text).lower()\n        text = re.sub(r'[^a-z\\s]', '', text)\n        return text.split()\n\n    def build_vocabulary(self, sentence_list):\n        frequencies = Counter()\n        idx = 4 # Started indexing after our 4 special tokens\n\n        for sentence in sentence_list:\n            for word in self.tokenizer(sentence):\n                frequencies[word] += 1\n\n                # Only added word to vocab if it meets the frequency threshold\n                if frequencies[word] == self.freq_threshold:\n                    self.stoi[word] = idx\n                    self.itos[idx] = word\n                    idx += 1\n\n    def numericalize(self, text):\n        tokenized_text = self.tokenizer(text)\n        # Converted each word to its ID; using <unk> if word isn't in vocab\n        return [\n            self.stoi.get(token, self.stoi[\"<unk>\"]) \n            for token in tokenized_text\n        ]\n\n# --- LOADING CAPTIONS ---\n\n# Flickr30k typically uses '|' as a delimiter in captions.txt\nCAPTIONS_PATH = '/kaggle/input/flickr30k/captions.txt'\ndf = pd.read_csv(CAPTIONS_PATH)\n\n# Robust column detection (handles varying column names like 'comment' or 'caption')\ncaption_col = 'comment' if 'comment' in df.columns else df.columns[-1]\nprint(f\"Using column '{caption_col}' for captions.\")\n\n# --- BUILDING VOCABULARY ---\n# We use a threshold of 5 to ignore rare words/typos and keep the model efficient\nvocab = Vocabulary(freq_threshold=5)\nvocab.build_vocabulary(df[caption_col].tolist())\n\nprint(f\"Vocabulary Size: {len(vocab)}\")\n\n# TEST\nsample_text = df.iloc[0][caption_col]\nnumerical_seq = [vocab.stoi[\"<start>\"]] + vocab.numericalize(sample_text) + [vocab.stoi[\"<end>\"]]\n\nprint(f\"\\nExample Pre-processing:\")\nprint(f\"Original: {sample_text}\")\nprint(f\"Tokenized: {vocab.tokenizer(sample_text)}\")\nprint(f\"Numericalized: {numerical_seq}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T07:22:43.098851Z","iopub.execute_input":"2026-02-12T07:22:43.099288Z","iopub.status.idle":"2026-02-12T07:22:44.745728Z","shell.execute_reply.started":"2026-02-12T07:22:43.099260Z","shell.execute_reply":"2026-02-12T07:22:44.744880Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part 3: The Seq2Seq Architecture","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass Encoder(nn.Module):\n    def __init__(self, embed_size):\n        super(Encoder, self).__init__()\n        # Projects the 2048-dim ResNet features into the hidden_size \n        self.fc = nn.Linear(2048, embed_size)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, features):\n        # features shape: (batch_size, 2048)\n        # Output shape: (batch_size, embed_size)\n        return self.dropout(self.relu(self.fc(features)))\n\nclass Decoder(nn.Module):\n    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n        super(Decoder, self).__init__()\n        # Input: Word Embeddings of the caption \n        self.embed = nn.Embedding(vocab_size, embed_size)\n        \n        # The LSTM storyteller \n        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n        \n        # Output: A Linear layer mapped to your vocab_size \n        self.linear = nn.Linear(hidden_size, vocab_size)\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, features, captions):\n        # features: encoder output, captions: tokenized sequences\n        # Create embeddings for the caption tokens\n        # We don't pass the <end> token to the LSTM during training\n        embeddings = self.dropout(self.embed(captions[:, :-1]))\n        \n        # Initial Hidden State logic: \n        # We treat the image features as the \"first word\" in the sequence\n        # features shape: (batch_size, embed_size) -> (batch_size, 1, embed_size)\n        embeddings = torch.cat((features.unsqueeze(1), embeddings), dim=1)\n        \n        # hiddens shape: (batch_size, seq_len, hidden_size)\n        hiddens, _ = self.lstm(embeddings)\n        \n        # Map LSTM outputs to vocabulary probabilities \n        outputs = self.linear(hiddens)\n        return outputs\n\nclass NeuralStoryteller(nn.Module):\n    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n        super(NeuralStoryteller, self).__init__()\n        self.encoder = Encoder(embed_size)\n        self.decoder = Decoder(embed_size, hidden_size, vocab_size, num_layers)\n\n    def forward(self, images, captions):\n        # images: pre-extracted 2048-dim vectors\n        features = self.encoder(images)\n        outputs = self.decoder(features, captions)\n        return outputs\n\n# --- INITIALIZING THE MODEL ---\n# Hyperparameters as suggested \nEMBED_SIZE = 512\nHIDDEN_SIZE = 512\nVOCAB_SIZE = len(vocab)\n\nmodel = NeuralStoryteller(EMBED_SIZE, HIDDEN_SIZE, VOCAB_SIZE).to(device)\n\nprint(f\"Model Architecture Initialized.\")\nprint(f\"Embed Size: {EMBED_SIZE}, Hidden Size: {HIDDEN_SIZE}, Vocab Size: {VOCAB_SIZE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T07:22:44.747387Z","iopub.execute_input":"2026-02-12T07:22:44.747827Z","iopub.status.idle":"2026-02-12T07:22:44.887980Z","shell.execute_reply.started":"2026-02-12T07:22:44.747801Z","shell.execute_reply":"2026-02-12T07:22:44.887338Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part 4: Training & Inference","metadata":{}},{"cell_type":"code","source":"from torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader\n\n# COLLATE FUNCTION: Handles variable length captions in a batch\nclass MyCollate:\n    def __init__(self, pad_idx):\n        self.pad_idx = pad_idx\n\n    def __call__(self, batch):\n        # batch: list of tuples (feature, numericalized_caption)\n        imgs = torch.stack([item[0] for item in batch])\n        targets = [torch.tensor(item[1]) for item in batch]\n        # Pad sequences so they all have the same length in the tensor\n        targets = pad_sequence(targets, batch_first=True, padding_value=self.pad_idx)\n        return imgs, targets\n\n# DATASET WRAPPER: Pairs your cached .pkl features with the captions\nclass CachedDataset(Dataset):\n    def __init__(self, df, features_dict, vocab, caption_col):\n        self.df = df\n        self.features_dict = features_dict\n        self.vocab = vocab\n        self.caption_col = caption_col\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        caption = self.df.iloc[idx][self.caption_col]\n        img_id = self.df.iloc[idx]['image'] # Ensure this matches your CSV column name\n        \n        feature = torch.tensor(self.features_dict[img_id])\n        \n        # Format: <start> caption <end>\n        numericalized = [self.vocab.stoi[\"<start>\"]]\n        numericalized += self.vocab.numericalize(str(caption))\n        numericalized += [self.vocab.stoi[\"<end>\"]]\n        \n        return feature, numericalized\n\n# --- INITIALIZING LOADERS ---\ntrain_dataset = CachedDataset(df, features_dict, vocab, caption_col)\ntrain_loader = DataLoader(\n    dataset=train_dataset,\n    batch_size=64,\n    shuffle=True,\n    num_workers=2,\n    collate_fn=MyCollate(pad_idx=vocab.stoi[\"<pad>\"])\n)\n\n# 3. LOSS & OPTIMIZER \ncriterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<pad>\"])\noptimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n\n# 4. INFERENCE: GREEDY SEARCH \ndef greedy_search(model, image_feature, vocab, max_len=20):\n    model.eval()\n    result_caption = []\n    \n    with torch.no_grad():\n        # Encoding the image feature\n        features = model.encoder(image_feature.to(device).view(1, -1)).unsqueeze(1)\n        states = None \n        \n        # Starting with <start> token\n        input_word = torch.tensor([vocab.stoi[\"<start>\"]]).to(device).unsqueeze(0)\n        \n        for i in range(max_len):\n            embeddings = model.decoder.embed(input_word)\n            \n            # For the very first step, concatenating the image context\n            if i == 0:\n                embeddings = torch.cat((features, embeddings), dim=1)\n            \n            hiddens, states = model.decoder.lstm(embeddings, states)\n            outputs = model.decoder.linear(hiddens[:, -1, :])\n            \n            predicted = outputs.argmax(dim=1)\n            \n            word = vocab.itos[predicted.item()]\n            if word == \"<end>\":\n                break\n                \n            result_caption.append(word)\n            input_word = predicted.unsqueeze(0)\n            \n    return \" \".join(result_caption)\n\nprint(\"Training components and Greedy Inference initialized.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T07:22:44.888906Z","iopub.execute_input":"2026-02-12T07:22:44.889138Z","iopub.status.idle":"2026-02-12T07:22:44.900969Z","shell.execute_reply.started":"2026-02-12T07:22:44.889116Z","shell.execute_reply":"2026-02-12T07:22:44.900405Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Training Loop**","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\n\ndef train_model(epochs=5):\n    model.train()\n    train_losses = []\n\n    for epoch in range(epochs):\n        running_loss = 0.0\n        loop = tqdm(enumerate(train_loader), total=len(train_loader))\n        \n        for idx, (features, captions) in loop:\n            features, captions = features.to(device), captions.to(device)\n\n            # Forward\n            outputs = model(features, captions)\n            \n            # Loss Calculation (Shift targets to align with predictions)\n            loss = criterion(outputs.view(-1, VOCAB_SIZE), captions.view(-1))\n\n            # Backward\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            loop.set_description(f\"Epoch [{epoch+1}/{epochs}]\")\n            loop.set_postfix(loss=loss.item())\n\n        train_losses.append(running_loss / len(train_loader))\n    \n    return train_losses\n\n# Start Training\nlosses = train_model(epochs=10) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T07:22:44.901733Z","iopub.execute_input":"2026-02-12T07:22:44.901958Z","iopub.status.idle":"2026-02-12T07:34:57.212167Z","shell.execute_reply.started":"2026-02-12T07:22:44.901939Z","shell.execute_reply":"2026-02-12T07:34:57.211408Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Beam Search Implementation**","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\n\ndef beam_search(model, image_feature, vocab, beam_width=5, max_len=20):\n    model.eval()\n    \n    with torch.no_grad():\n        feature_context = model.encoder(image_feature.to(device).view(1, -1)).unsqueeze(1)\n        \n        # Candidates: (score, sequence, hidden_states)\n        candidates = [(0.0, [vocab.stoi[\"<start>\"]], None)]\n        \n        for i in range(max_len):\n            all_candidates = []\n            \n            for score, seq, states in candidates:\n                if seq[-1] == vocab.stoi[\"<end>\"]:\n                    all_candidates.append((score, seq, states))\n                    continue\n                \n                input_word = torch.tensor([seq[-1]]).to(device).view(1, 1)\n                embeddings = model.decoder.embed(input_word)\n                \n                if i == 0:\n                    embeddings = torch.cat((feature_context, embeddings), dim=1)\n                \n                hiddens, states = model.decoder.lstm(embeddings, states)\n                \n                # CRITICAL FIX: Only take the LAST output vector [:, -1, :]\n                outputs = model.decoder.linear(hiddens[:, -1, :])\n                log_probs = F.log_softmax(outputs, dim=1)\n                \n                top_log_probs, top_indices = log_probs.topk(beam_width)\n                \n                for j in range(beam_width):\n                    next_score = score + top_log_probs[0][j].item()\n                    next_seq = seq + [top_indices[0][j].item()]\n                    all_candidates.append((next_score, next_seq, states))\n            \n            # Sort and prune to beam_width\n            candidates = sorted(all_candidates, key=lambda x: x[0], reverse=True)[:beam_width]\n            \n            if all(c[1][-1] == vocab.stoi[\"<end>\"] for c in candidates):\n                break\n        \n        best_seq = candidates[0][1]\n        final_caption = [vocab.itos[idx] for idx in best_seq if idx not in [vocab.stoi[\"<start>\"], vocab.stoi[\"<end>\"]]]\n        \n        return \" \".join(final_caption)\n             \n        \n\n# --- QUICK TEST---\nsample_feat = torch.tensor(features_dict[list(features_dict.keys())[0]])\nprint(\"Greedy:\", greedy_search(model, sample_feat, vocab))\nprint(\"Beam (k=5):\", beam_search(model, sample_feat, vocab, beam_width=5))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T07:34:57.213646Z","iopub.execute_input":"2026-02-12T07:34:57.213916Z","iopub.status.idle":"2026-02-12T07:34:57.489929Z","shell.execute_reply.started":"2026-02-12T07:34:57.213887Z","shell.execute_reply":"2026-02-12T07:34:57.489289Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Deliverable 1: Caption Examples","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom PIL import Image\nimport random\nimport os\n\ndef display_caption_examples(model, features_dict, df, vocab, img_dir, num_examples=5):\n    # Selecting random image names from our feature cache\n    all_image_names = list(features_dict.keys())\n    random_images = random.sample(all_image_names, num_examples)\n    \n    # Setup the plot\n    plt.figure(figsize=(20, 20))\n    \n    for i, img_name in enumerate(random_images):\n        # 1. Loading the actual image file\n        img_path = os.path.join(img_dir, img_name)\n        image = Image.open(img_path).convert(\"RGB\")\n        \n        # 2. Getting the Ground Truth (taking the first of the 5 available captions)\n        # Using the column name identified in Phase 2\n        ground_truth = df[df['image'] == img_name].iloc[0][caption_col]\n        \n        # 3. Generating the Model Caption\n        # Convert cached feature back to tensor\n        feature_tensor = torch.tensor(features_dict[img_name])\n        model_caption = greedy_search(model, feature_tensor, vocab)\n        \n        # 4. Plotting\n        plt.subplot(num_examples, 1, i + 1)\n        plt.imshow(image)\n        plt.title(f\"Ground Truth: {ground_truth}\\nModel Generated: {model_caption}\", \n                  fontsize=12, pad=10, loc='left', color='blue' if i % 2 == 0 else 'green')\n        plt.axis('off')\n        \n    plt.tight_layout()\n    plt.show()\n\n# Runing the visual evaluation \n# Ensuring IMAGE_DIR is the path found in Phase 1\ndisplay_caption_examples(model, features_dict, df, vocab, IMAGE_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T07:34:57.490786Z","iopub.execute_input":"2026-02-12T07:34:57.491086Z","iopub.status.idle":"2026-02-12T07:34:58.436133Z","shell.execute_reply.started":"2026-02-12T07:34:57.491061Z","shell.execute_reply":"2026-02-12T07:34:58.435179Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Deliverable 2: Loss Curve","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_training_results(train_losses):\n    plt.figure(figsize=(10, 6))\n    plt.plot(train_losses, label='Training Loss', color='#1f77b4', linewidth=2)\n    plt.title('Deliverable 2: Model Training Progress', fontsize=14)\n    plt.xlabel('Epochs', fontsize=12)\n    plt.ylabel('Loss (CrossEntropy)', fontsize=12)\n    plt.legend()\n    plt.grid(True, linestyle='--', alpha=0.6)\n    plt.show()\n\n# Assuming 'losses' is the variable returned from your train_model function\nplot_training_results(losses)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T07:34:58.437610Z","iopub.execute_input":"2026-02-12T07:34:58.437868Z","iopub.status.idle":"2026-02-12T07:34:58.588243Z","shell.execute_reply.started":"2026-02-12T07:34:58.437844Z","shell.execute_reply":"2026-02-12T07:34:58.587647Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Deliverable 3: Quantative Evaluation","metadata":{}},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\ndef calculate_metrics(model, features_dict, df, vocab, num_test=100):\n    model.eval()\n    total_bleu = 0\n    smooth = SmoothingFunction().method1\n    \n    all_precision = []\n    all_recall = []\n    \n    # Evaluating on a representative subset of data\n    test_ids = list(features_dict.keys())[:num_test]\n    \n    for img_id in test_ids:\n        # 1. Getting references (ground truths)\n        references = df[df['image'] == img_id]['caption'].astype(str).tolist()\n        ref_tokens = [r.lower().split() for r in references]\n        \n        # 2. Getting model prediction\n        feat = torch.tensor(features_dict[img_id])\n        prediction = greedy_search(model, feat, vocab).split()\n        \n        # 3. Calculating BLEU-4\n        total_bleu += sentence_bleu(ref_tokens, prediction, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth)\n        \n        # 4. Calculating Precision/Recall (token-level)\n        gt_set = set(ref_tokens[0]) # comparing against first ground truth\n        pred_set = set(prediction)\n        common = gt_set & pred_set\n        \n        precision = len(common) / len(pred_set) if len(pred_set) > 0 else 0\n        recall = len(common) / len(gt_set) if len(gt_set) > 0 else 0\n        \n        all_precision.append(precision)\n        all_recall.append(recall)\n\n    avg_bleu = total_bleu / num_test\n    avg_p = sum(all_precision) / len(all_precision)\n    avg_r = sum(all_recall) / len(all_recall)\n    f1 = 2 * (avg_p * avg_r) / (avg_p + avg_r) if (avg_p + avg_r) > 0 else 0\n    \n    print(f\"--- Deliverable 3: Quantitative Metrics (n={num_test}) ---\")\n    print(f\"Average BLEU-4: {avg_bleu:.4f}\")\n    print(f\"Average Precision: {avg_p:.4f}\")\n    print(f\"Average Recall: {avg_r:.4f}\")\n    print(f\"Average F1-Score: {f1:.4f}\")\n\ncalculate_metrics(model, features_dict, df, vocab)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T07:34:58.590165Z","iopub.execute_input":"2026-02-12T07:34:58.590774Z","iopub.status.idle":"2026-02-12T07:35:01.426144Z","shell.execute_reply.started":"2026-02-12T07:34:58.590747Z","shell.execute_reply":"2026-02-12T07:35:01.425418Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), 'model.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T07:36:03.370881Z","iopub.execute_input":"2026-02-12T07:36:03.371198Z","iopub.status.idle":"2026-02-12T07:36:03.465175Z","shell.execute_reply.started":"2026-02-12T07:36:03.371170Z","shell.execute_reply":"2026-02-12T07:36:03.464314Z"}},"outputs":[],"execution_count":null}]}